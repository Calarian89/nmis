{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database if not exists\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS infra_intel_reporting\")\n",
    "\n",
    "# Step 1: Create a temporary view for switch interfaces\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW switch_interfaces AS\n",
    "    SELECT \n",
    "        lw.device_name, \n",
    "        DATE(lw.event_timestamp) AS event_date, \n",
    "        lw.device_type, \n",
    "        lw.region, \n",
    "        lw.location, \n",
    "        di.int_type, \n",
    "        di.int_last_change, \n",
    "        di.interface_desc, \n",
    "        di.description, \n",
    "        di.int_oper_status, \n",
    "        lw.host_server\n",
    "    FROM edl.nmis9_device_events lw\n",
    "    JOIN edl.nmis9_device_interface_events di ON lw.uuid = di.uuid\n",
    "    WHERE lw.event_timestamp >= (current_timestamp() - INTERVAL 30 DAYS)\n",
    "      AND lw.device_type = 'Switch'\n",
    "      AND di.int_type = 'ethernetCsmacd'\n",
    "      AND (di.int_oper_status = 'up' OR di.interface_desc != 'GigabitEthernet0/0')\n",
    "      AND di.interface_desc NOT IN ('Bluetooth0/4', 'null')\n",
    "      AND di.interface_desc NOT LIKE '%AppGigabitEthernet%'\n",
    "      AND lw.location != 'Unknown'\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Create a temporary view for latest record per day with aggregation\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW latest_record_per_day AS\n",
    "    SELECT\n",
    "        si.device_name,\n",
    "        si.event_date,\n",
    "        si.device_type,\n",
    "        si.region,\n",
    "        si.location,\n",
    "        si.int_type,\n",
    "        si.interface_desc,\n",
    "        MAX(si.int_last_change) as int_last_change,\n",
    "        MAX(si.description) as description,\n",
    "        MAX(si.int_oper_status) as int_oper_status,\n",
    "        si.host_server\n",
    "    FROM switch_interfaces si\n",
    "    GROUP BY\n",
    "        si.device_name,\n",
    "        si.event_date,\n",
    "        si.device_type,\n",
    "        si.region,\n",
    "        si.location,\n",
    "        si.int_type,\n",
    "        si.interface_desc,\n",
    "        si.host_server\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "latest_record_per_day_df = spark.sql(\"SELECT * FROM latest_record_per_day\")\n",
    "latest_record_per_day_df.persist()\n",
    "\n",
    "# Step 3: Create a temporary view for status tracker\n",
    "latest_record_per_day_df.createOrReplaceTempView(\"latest_record_per_day_persisted\")\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW status_tracker AS\n",
    "    SELECT\n",
    "        lr.device_name,\n",
    "        lr.event_date,\n",
    "        lr.device_type,\n",
    "        lr.region,\n",
    "        lr.location,\n",
    "        lr.int_type,\n",
    "        lr.interface_desc,\n",
    "        lr.int_last_change,\n",
    "        lr.description,\n",
    "        lr.int_oper_status,\n",
    "        lr.host_server,\n",
    "        LAG(lr.int_oper_status) OVER (PARTITION BY lr.device_name, lr.interface_desc ORDER BY lr.event_date) as prev_int_oper_status\n",
    "    FROM latest_record_per_day_persisted lr\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 4: Create a temporary view for status groups\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW status_groups AS\n",
    "    SELECT\n",
    "        st.device_name,\n",
    "        st.event_date,\n",
    "        st.device_type,\n",
    "        st.region,\n",
    "        st.location,\n",
    "        st.int_type,\n",
    "        st.interface_desc,\n",
    "        st.int_last_change,\n",
    "        st.description,\n",
    "        st.int_oper_status,\n",
    "        st.host_server,\n",
    "        st.prev_int_oper_status,\n",
    "        CASE \n",
    "            WHEN st.prev_int_oper_status = st.int_oper_status THEN 0\n",
    "            ELSE 1\n",
    "        END AS status_switch,\n",
    "        SUM(CASE \n",
    "            WHEN st.prev_int_oper_status = st.int_oper_status THEN 0\n",
    "            ELSE 1\n",
    "        END) OVER (PARTITION BY st.device_name, st.interface_desc ORDER BY st.event_date) AS status_group\n",
    "    FROM status_tracker st\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 5: Create a temporary view for consecutive days\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW consecutive_days AS\n",
    "    SELECT\n",
    "        sg.device_name, \n",
    "        sg.event_date, \n",
    "        sg.device_type, \n",
    "        sg.region, \n",
    "        sg.location, \n",
    "        sg.int_type, \n",
    "        sg.interface_desc,\n",
    "        sg.int_last_change,\n",
    "        sg.description,\n",
    "        sg.host_server,\n",
    "        sg.int_oper_status,\n",
    "        MIN(sg.event_date) OVER (PARTITION BY sg.device_name, sg.interface_desc, sg.status_group) AS min_event_date,\n",
    "        COUNT(*) OVER (PARTITION BY sg.device_name, sg.interface_desc, sg.status_group ORDER BY sg.event_date) AS consecutive_days\n",
    "    FROM status_groups sg\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Persist the intermediate result to avoid recomputation\n",
    "consecutive_days_df = spark.sql(\"SELECT * FROM consecutive_days\")\n",
    "consecutive_days_df.persist()\n",
    "\n",
    "# Step 6: Create the final DataFrame\n",
    "consecutive_days_df.createOrReplaceTempView(\"consecutive_days_persisted\")\n",
    "df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        cd.device_name, \n",
    "        cd.event_date, \n",
    "        cd.device_type, \n",
    "        cd.region, \n",
    "        cd.location, \n",
    "        cd.int_type, \n",
    "        cd.interface_desc,\n",
    "        cd.int_last_change,\n",
    "        cd.description,\n",
    "        cd.host_server,\n",
    "        cd.int_oper_status,\n",
    "        CASE \n",
    "            WHEN cd.consecutive_days < 30 THEN cd.consecutive_days\n",
    "            ELSE 30\n",
    "        END AS filtered_consecutive_days,\n",
    "        CASE \n",
    "            WHEN cd.int_oper_status = 'down' AND \n",
    "                  CAST(REGEXP_EXTRACT(cd.int_last_change, '([0-9]+) days') AS INT) >= 90 \n",
    "                  THEN CAST(REGEXP_EXTRACT(cd.int_last_change, '([0-9]+) days') AS STRING) \n",
    "            ELSE NULL \n",
    "        END AS `90+ days`\n",
    "    FROM consecutive_days_persisted cd\n",
    "    ORDER BY cd.event_date DESC\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Write the DataFrame to the specified path and save as a table\n",
    "df.write.option(\n",
    "    \"path\", \"/mnt/sandbox/AWS-EDL-INFRA-INTEL-DATA/reporting/nmis_interface_metrics\"\n",
    ").saveAsTable(\n",
    "    \"infra_intel_reporting.nmis_interface_metrics\",\n",
    "    format=\"parquet\",\n",
    "    mode=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "CREATE OR REPLACE VIEW edl_views.nmis9_report\n",
    "TBLPROPERTIES (\n",
    "    \"edl_sources\" = \"com.deere.enterprise.datalake.enhance.iit_reporting\") AS\n",
    "select * from infra_intel_reporting.nmis_interface_metrics\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
