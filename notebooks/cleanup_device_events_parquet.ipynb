{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read in current device_events into a Dataframe\n",
    "df = spark.read.parquet(\"/mnt/edl/raw/nmis_dc_logs/device_events.parquet\")\n",
    "\n",
    "# Append the current results into the archived device_events parquet\n",
    "# TODO clean out this archive over a longer duration.\n",
    "df.write.mode(\"append\").parquet(\n",
    "    \"/mnt/edl/raw/nmis_dc_logs/archive/device_events.parquet\"\n",
    ")\n",
    "\n",
    "# Get timestamps for the last 15 days\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=15)\n",
    "\n",
    "# Filter the current results to include only the events from last 15 days\n",
    "df = df.filter(\n",
    "    (F.col(\"event_timestamp\") >= start_date) & (F.col(\"event_timestamp\") <= end_date)\n",
    ")\n",
    "\n",
    "# Overwrite the current device_events with the results from last 15 days.\n",
    "df.write.mode(\"overwrite\").parquet(\"/mnt/edl/raw/nmis_dc_logs/device_events.parquet\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
